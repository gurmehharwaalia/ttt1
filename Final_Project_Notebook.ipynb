{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOz3T51aEew1MogARWdVOUf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gurmehharwaalia/ttt1/blob/main/Final_Project_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üöÄMachine Learning Project: Predicting Diabetes Progression**\n",
        "\n",
        " This notebook presents a complete analysis for predicting diabetes disease progression using machine learning techniques. The analysis follows the required structure: problem definition, exploratory data analysis, modeling with regularization, evaluation, interpretation, and conclusions.\n",
        "\n",
        "The problem is framed as a regression task using the Diabetes dataset from scikit-learn.\n",
        "\n",
        "üìä1.**Problem Definition and Context**\n",
        "\n",
        "**Problem Statement**\n",
        "\n",
        "This is a regression problem, where we aim to predict a continuous value: the quantitative measure of disease progression one year after baseline for diabetes patients.\n",
        "The target variable is a continuous score representing disease progression, based on various physiological features.\n",
        "\n",
        "\n",
        "**Data Source and Variables**\n",
        "\n",
        "The data is from the scikit-learn load_diabetes dataset, which includes 442 samples with 10 baseline variables:\n",
        "\n",
        "age: Age in years\n",
        "\n",
        "sex: Sex (binary)\n",
        "\n",
        "bmi: Body mass index\n",
        "\n",
        "bp: Average blood pressure\n",
        "\n",
        "s1: Total serum cholesterol (tc)\n",
        "\n",
        "s2: Low-density lipoproteins (ldl)\n",
        "\n",
        "s3: High-density lipoproteins (hdl)\n",
        "\n",
        "s4: Total cholesterol / HDL (tch)\n",
        "\n",
        "s5: Log of serum triglycerides level (ltg)\n",
        "\n",
        "s6: Blood sugar level (glu)\n",
        "\n",
        "The target is a continuous value measuring disease progression.\n",
        "Data source: Efron et al., \"Least Angle Regression,\" Annals of Statistics (2004).\n",
        "\n",
        "# 1.3 **Relevance**\n",
        "\n",
        "This problem is relevant for healthcare and medical research. Predicting disease progression can help doctors identify high-risk patients early, enabling personalized treatment plans, resource allocation, and potentially reducing healthcare costs. For businesses in health tech or pharmaceuticals, such models can inform drug development or patient monitoring apps.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KlYk1ZTBRqD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîç2. Exploratory Data Analysis (EDA)\n",
        "In this section, we load the data, inspect its quality, describe features, and visualize relationships to inform modeling decisions.\n",
        "\n",
        "# 2.1 Data Loading\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "k7r9Y5aRRxMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "diabetes = load_diabetes()\n",
        "X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
        "y = pd.Series(diabetes.target, name='target')\n",
        "\n",
        "# Combine into a single DataFrame for EDA\n",
        "df = pd.concat([X, y], axis=1)\n",
        "\n",
        "# Display first few rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "4ItOtL-1R09i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2 Data Quality Inspection\n",
        "2.2.1 Missing Values Detection"
      ],
      "metadata": {
        "id": "yELqarBJR3Es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic info\n",
        "df.info()\n",
        "\n",
        "# Summary statistics\n",
        "df.describe()\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values:\\n\", df.isnull().sum())\n",
        "\n",
        "# Check for duplicates\n",
        "print(\"Duplicates:\", df.duplicated().sum())\n",
        "\n",
        "# Outliers: Using boxplots for numerical features\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(data=df.drop('sex', axis=1))  # sex is binary\n",
        "plt.title('Boxplots for Numerical Features')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Note: All features are already scaled (mean 0, std 1) in this dataset, no missing values or duplicates."
      ],
      "metadata": {
        "id": "0JZ_CyKVR467"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distributions of features\n",
        "df.hist(bins=20, figsize=(12, 10))\n",
        "plt.suptitle('Distributions of Features and Target')\n",
        "plt.show()\n",
        "\n",
        "# Correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n",
        "\n",
        "# Relationships with target\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, col in enumerate(X.columns):\n",
        "    plt.subplot(3, 4, i+1)\n",
        "    sns.scatterplot(x=df[col], y=df['target'])\n",
        "    plt.title(col)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KU1Om1_1Esld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights from EDA:\n",
        "\n",
        "Strong positive correlations with target: bmi (0.59), s5 (0.57).\n",
        "\n",
        "Multicollinearity: s1 and s2 (0.90), s4 and s3 (-0.74).\n",
        "\n",
        "This suggests regularization is useful to handle multicollinearity and prevent overfitting.\n",
        "\n",
        "Feature engineering: No need for much, as data is pre-scaled.\n",
        "\n",
        "Model choice: Linear models may work, but non-linear (e.g., trees) could capture complex relationships."
      ],
      "metadata": {
        "id": "N6VrjTaNEvHr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öôÔ∏è 3. Modeling\n",
        "We split the data, then train: baseline (LinearRegression), regularized (Ridge, Lasso), and a new model (RandomForestRegressor).\n",
        "\n",
        "\n",
        "# 3.1 Train/Test Split Summary"
      ],
      "metadata": {
        "id": "1EL41A7DE3C9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data: 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "shVwNnaLE4DS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.2 Model Training\n",
        "\n",
        "# 3.2.1 Regularization Approach\n",
        "\n",
        "Regularization is useful here due to multicollinearity (e.g., between s1 and s2) and to prevent overfitting with 10 features on 442 samples.\n",
        "\n",
        "We'll use Ridge (L2) and Lasso (L1). Hyperparameters: alpha for strength of regularization.\n",
        "\n",
        "# 3.2.2 Create Models"
      ],
      "metadata": {
        "id": "63aCZtqqE53s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Baseline: Linear Regression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train, y_train)\n",
        "\n",
        "# Regularized: Ridge\n",
        "ridge_params = {'alpha': np.logspace(-4, 4, 20)}\n",
        "ridge_grid = GridSearchCV(Ridge(), ridge_params, cv=5, scoring='neg_mean_squared_error')\n",
        "ridge_grid.fit(X_train, y_train)\n",
        "best_ridge = ridge_grid.best_estimator_\n",
        "\n",
        "# Lasso\n",
        "lasso_params = {'alpha': np.logspace(-4, 4, 20)}\n",
        "lasso_grid = GridSearchCV(Lasso(), lasso_params, cv=5, scoring='neg_mean_squared_error')\n",
        "lasso_grid.fit(X_train, y_train)\n",
        "best_lasso = lasso_grid.best_estimator_"
      ],
      "metadata": {
        "id": "9k6FngoFFFDl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2 Cross-Validation"
      ],
      "metadata": {
        "id": "JrSItBmBRHTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CV scores for baseline\n",
        "lin_cv_scores = cross_val_score(lin_reg, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "print(\"Linear CV MSE mean:\", -lin_cv_scores.mean(), \"std:\", lin_cv_scores.std())\n",
        "\n",
        "# For best Ridge and Lasso, from grid search\n",
        "print(\"Best Ridge alpha:\", ridge_grid.best_params_['alpha'], \"CV MSE:\", -ridge_grid.best_score_)\n",
        "print(\"Best Lasso alpha:\", lasso_grid.best_params_['alpha'], \"CV MSE:\", -lasso_grid.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odWobrCdRJTW",
        "outputId": "b8fe65e3-00eb-40f3-e3fe-b9c6d86ccf92"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear CV MSE mean: 3143.0153074277237 std: 355.46681369850495\n",
            "Best Ridge alpha: 0.08858667904100823 CV MSE: 3123.40709108382\n",
            "Best Lasso alpha: 0.08858667904100823 CV MSE: 3126.56661789274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3 One New Model Not Covered in Class\n",
        "We'll use Random Forest Regressor."
      ],
      "metadata": {
        "id": "SmyA1l7TRLwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Random Forest\n",
        "rf_params = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}\n",
        "rf_grid = GridSearchCV(RandomForestRegressor(random_state=42), rf_params, cv=5, scoring='neg_mean_squared_error')\n",
        "rf_grid.fit(X_train, y_train)\n",
        "best_rf = rf_grid.best_estimator_\n",
        "print(\"Best RF params:\", rf_grid.best_params_, \"CV MSE:\", -rf_grid.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RNYwv5KRNWR",
        "outputId": "3427877b-b740-4e74-f707-24f37b6c0cf1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best RF params: {'max_depth': 20, 'n_estimators': 200} CV MSE: 3469.952781526771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.4 Model Comparison"
      ],
      "metadata": {
        "id": "NQbyN4XhRPCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions and metrics\n",
        "models = {'Linear': lin_reg, 'Ridge': best_ridge, 'Lasso': best_lasso, 'RandomForest': best_rf}\n",
        "\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    results.append({'Model': name, 'MSE': mse, 'RMSE': rmse, 'R2': r2})\n",
        "\n",
        "# Table\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Model', y='R2', data=results_df)\n",
        "plt.title('Model Comparison by R2 Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qkK7uXL-RRi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Interpretation & Goodness-of-Fit\n",
        "Model Parameters"
      ],
      "metadata": {
        "id": "6cicTlrZRTzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coefficients for Ridge (example)\n",
        "print(\"Ridge Coefficients:\\n\", pd.Series(best_ridge.coef_, index=X.columns))\n",
        "\n",
        "# Feature importances for RF\n",
        "importances = pd.Series(best_rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=importances.values, y=importances.index)\n",
        "plt.title('Random Forest Feature Importances')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6Q_5tcYHRVi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goodness-of-Fit"
      ],
      "metadata": {
        "id": "H9gBuPW9RXEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Residuals for best model (RF)\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "residuals = y_test - y_pred_rf\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=y_pred_rf, y=residuals)\n",
        "plt.axhline(0, color='r', linestyle='--')\n",
        "plt.title('Residual Plot for Random Forest')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Residuals')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FJYp4UY7RYaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Predictions on New Data"
      ],
      "metadata": {
        "id": "jZOATdSvRaQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate new data (mean values, perturb some)\n",
        "new_data = X_test.iloc[:5].copy()  # Example from test\n",
        "new_data['bmi'] += 0.05  # Increase bmi\n",
        "\n",
        "# Predictions\n",
        "new_preds = best_rf.predict(new_data)\n",
        "\n",
        "# Display\n",
        "pred_df = pd.DataFrame({'Actual': y_test.iloc[:5].values, 'Predicted': new_preds})\n",
        "pred_df\n",
        "\n",
        "# Plot\n",
        "pred_df.plot(kind='bar', figsize=(8, 6))\n",
        "plt.title('Example Predictions vs Actuals')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OMI3qfzFRb-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Final Conclusions and Recommendations\n",
        "Conclusions: We built models to predict diabetes progression, with Random Forest outperforming linear variants (R2 ~0.45). Key features: bmi, s5.\n",
        "Recommendations: Focus interventions on weight management (bmi). Further work: Add more features (e.g., diet), try deep learning."
      ],
      "metadata": {
        "id": "QgEgL0Y6Rd9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "U8fOjxb4dTHs"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}